{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Needed packages to run everything\"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "import yaml\n",
    "\n",
    "\"\"\"General methods that are dictate what stable version the file is currently in, how the file is read, created and how the metadata is dumped into it\"\"\"\n",
    "\n",
    "\"\"\"Global source systems used in datasets\"\"\"\n",
    "sources = []\n",
    "\n",
    "class generalMethods:\n",
    "    \n",
    "    def __init__(self, majorVersion: int, minorVersion : int) -> None:\n",
    "        \"\"\"\n",
    "        Constructor of the class general methods \n",
    "        ...\n",
    "        Arguments\n",
    "        ----------\n",
    "        self: \n",
    "            variable that calls this class to create an instance\n",
    "        majorVersion : int\n",
    "            current major version that is stable for instance\n",
    "        minorVersion : int\n",
    "            current minor version that is stable for instance\n",
    "        ...\n",
    "        Returns\n",
    "        ----------\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.majorVersion = majorVersion\n",
    "        self.minorVersion = minorVersion\n",
    "        self.stableVersion = f\"{self.majorVersion}.{self.minorVersion}\"\n",
    "\n",
    "    def writeMetadataFile(self, schema: dict, outputMetadataPath: str, fileName: str, outputFileType : str = \"yaml\") -> None:\n",
    "        \"\"\"\n",
    "        Write generated metadata, with the current stable version\n",
    "        ...\n",
    "        Arguments\n",
    "        ----------\n",
    "        schema : dict\n",
    "            dictionary that contains all the data that needs to be inputed inside the .yaml file to structure the metadata\n",
    "        outputMetadataPath : str\n",
    "            path of where the metadata files are going to be saved, so that they are structured and easy to find\n",
    "        fileName : str\n",
    "            path of where each folder is gonna be grouped based on the type of the file\n",
    "        stableVersion : str\n",
    "            latest stable version that the metadata will be generated, which is depended on the global variables majorVersion and minorVersion\n",
    "        outputFileType : str\n",
    "            output file extension (type) that we want to create, default value is `yaml` if no value is given for that parameter\n",
    "        ...\n",
    "        Returns\n",
    "        ----------\n",
    "        None\n",
    "        \"\"\"\n",
    "        with open(f'{outputMetadataPath}/{fileName}/{fileName}%{self.stableVersion}.{outputFileType}', 'w') as file:\n",
    "            yaml.dump(schema, file, sort_keys=False, version=(self.majorVersion, self.minorVersion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Default functions to check if certain values fulfill certain criteria and functions that manipulate with the global \"sources\" variable\"\"\"\n",
    "\n",
    "\"\"\"Mapping string value to boolean so we can use where ever they used\"\"\"\n",
    "strBoolMapping = {\n",
    "    \"y\": True,\n",
    "    \"n\": False,\n",
    "    \"gdpr\": \"true\"\n",
    "}\n",
    "\n",
    "def isNull(nullable : str) -> bool:\n",
    "    \"\"\"\n",
    "    Finding from the dataframe column if that column can have null values (is it required or not)\n",
    "    ...\n",
    "    Arguments\n",
    "    ----------\n",
    "    nullable : str\n",
    "        cell value of the row that we are currently checking, on the column \"NULLABLE\"\n",
    "    ...\n",
    "    Returns\n",
    "    ----------\n",
    "    Returns a boolean value, based on the variable strBoolMapping\n",
    "    \"\"\"\n",
    "    return strBoolMapping[nullable.lower()]\n",
    "\n",
    "def isDate(dataType : str) -> bool:\n",
    "    \"\"\"\n",
    "    Function returns extra metadata for column \"DATA_TYPE\", if that column fulfills a certain condition (is a date type)\n",
    "    ...\n",
    "    Arguments\n",
    "    ----------\n",
    "    dataType : str\n",
    "        cell value of the row that we are currently checking, on the column \"DATA_TYPE\"\n",
    "    ...\n",
    "    Returns\n",
    "    ----------\n",
    "    Retuns a boolean value, if that specific value has type date\n",
    "    \"\"\"\n",
    "    dateTypeChecked = \"date\"\n",
    "    dateBool = dataType.lower() == dateTypeChecked\n",
    "    return dateBool\n",
    "\n",
    "def isGdpr(gdprFlag : str) -> bool:\n",
    "    \"\"\"\n",
    "    Function returns extra metadata for column \"GDPR_FLAG\", if that row fulfills a certain condition (has the cell value \"true\")\n",
    "    ...\n",
    "    Arguments\n",
    "    ----------\n",
    "    gdprFlag : str\n",
    "        cell value of the row that we are currently checking, on the column \"GDPR_FLAG\"\n",
    "    ...\n",
    "    Returns\n",
    "    ----------\n",
    "    Boolean value, if the value that we want to compare is equal that to the value that is being compared by \n",
    "    \"\"\"\n",
    "    gdprBool = gdprFlag.lower() == strBoolMapping[\"gdpr\"]\n",
    "    return gdprBool\n",
    "\n",
    "\"\"\"Defining the basic functions for reading and specifying the current stable version for the generated metadata file\"\"\"\n",
    "def insertSourceSystem(source : dict) -> None:\n",
    "    \"\"\"\n",
    "    Inserting all the possible source systems into a single global variable\n",
    "    ...\n",
    "    Arguments\n",
    "    ----------\n",
    "    source : str\n",
    "        source is the dictionary that a defines the source of a specific table\n",
    "    ...\n",
    "    Returns\n",
    "    ----------\n",
    "    None\n",
    "    \"\"\"\n",
    "    sources.append(source)\n",
    "\n",
    "def getSourceSystem(sourceRef: int) -> dict:\n",
    "    return sources[sourceRef]\n",
    "\n",
    "def isDistinctSourceSystem(source : str) -> bool:\n",
    "    \"\"\"\n",
    "    Determining if the source system that we are checking has already appeard, or is it the first occurence of this value of source system\n",
    "    ...\n",
    "    Arguments\n",
    "    ----------\n",
    "    sourceSystem : str\n",
    "        name of the source system that we are checking\n",
    "    ...\n",
    "    Returns \n",
    "    ----------\n",
    "    Boolean value, based on the condition that the source system that we are checking hasn't appeared before\n",
    "    \"\"\"\n",
    "    sourceSystemBool = source not in sources\n",
    "    return sourceSystemBool\n",
    "\n",
    "def createSourceSystemRef(fileName : str, decimalSeparator : str, fileType : str) -> int:\n",
    "    \"\"\"\n",
    "    Creating the yaml anchor reference, so that aliases are created faster\n",
    "    ...\n",
    "    Arguments\n",
    "    ----------\n",
    "    sourceSystem : str\n",
    "        name of the source system that we want to create reference to\n",
    "    fileName : str\n",
    "        name of the file that this table column is part of\n",
    "    decimalSeparator : str\n",
    "        value that determines how the decimals are separated\n",
    "    fileType : str\n",
    "        the file type that this table column is part of\n",
    "    ...\n",
    "    Returns\n",
    "    ----------\n",
    "    Index of element that corresponds with source in the list of sources\n",
    "    \"\"\"\n",
    "    source = {\n",
    "        \"load_type\": \"delta\",\n",
    "        \"file_pattern\": fileName,\n",
    "        \"params\": {\n",
    "            \"decimal_separator\": decimalSeparator,\n",
    "            \"format\": fileType.lower()\n",
    "        }\n",
    "    }\n",
    "    if isDistinctSourceSystem(source):\n",
    "        insertSourceSystem(source)\n",
    "    return sources.index(source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Needed packages to run everything in this file\"\"\"\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "\"\"\"Functions that generate all the metadata for different parts of a dataframe \"\"\"\n",
    "    \n",
    "def genPrimaryArgsMeta(title : str, sourceSystem : str, description : str = \"A metaschema generated from an example dataset.\") -> dict:\n",
    "    \"\"\"\n",
    "    Write general info of metadata file\n",
    "    ...\n",
    "    Arguments\n",
    "    ----------\n",
    "    title : str\n",
    "        variable that is gonna be used for the title of the metadata file\n",
    "    sourceSystem : \n",
    "    description : str\n",
    "        variable that is gonna be used for the description of the metadata file, also has a default value which is gonna be used when there is not value give\n",
    "    ...\n",
    "    Returns\n",
    "    ----------\n",
    "    It returns a dictionary of the values mentioned above which are gonna be used later to build the entires metadata file\n",
    "    \"\"\"\n",
    "    version = 2\n",
    "    primaryArgSchema = {\n",
    "        \"version\": version,\n",
    "        \"source_system\": sourceSystem,\n",
    "        \"title\": title,\n",
    "        \"description\": f\"{description}\",\n",
    "        \"tables\": []\n",
    "    }\n",
    "    return primaryArgSchema\n",
    "    \n",
    "def genTableArgsMeta(tableName : str, sourceRef : int, inputSchema : list[dict]) -> dict:\n",
    "    \"\"\"\n",
    "    Generating primary attributes for metadata schema\n",
    "    ...\n",
    "    Arguments\n",
    "    ----------\n",
    "    tableName : str\n",
    "        name of the table that we are generating metadata for\n",
    "    sourceSystem : str\n",
    "        name of the source system that the table column is part of\n",
    "    inputSchema : list[dict]\n",
    "        list of dictionaries that are placed in the `columns` attribute\n",
    "    ...\n",
    "    Returns\n",
    "    ----------\n",
    "    A dictionary of the primary metadata attributes for the table that we are checking \n",
    "    \"\"\"\n",
    "    tableArgMeta = {\n",
    "        \"name\": f\"{tableName}\",\n",
    "        \"source\": getSourceSystem(sourceRef),\n",
    "        \"columns\": inputSchema,\n",
    "        \"primary_key\": \"unid\",\n",
    "        \"cdc_column\": \"dml_flag\",\n",
    "        \"cdc_type\": \"soft\" \n",
    "    }\n",
    "    return tableArgMeta\n",
    "\n",
    "def genTableArgsGdpr(tableName : str, inputSchema : list[dict]) -> dict:\n",
    "    \"\"\"\n",
    "    Generating primary attributes for gdpr schema\n",
    "    ...\n",
    "    Arguments\n",
    "    ----------\n",
    "    tableName : str\n",
    "        name of the table that we are generating metadata for\n",
    "    inputSchema : list[dict]\n",
    "        list of dictionaries that are placed in the `personal < hash` attribute\n",
    "    ...\n",
    "    Returns\n",
    "    ----------\n",
    "    A dictionary of the primary gdpr attributes for the table that we are checking \n",
    "    \"\"\"\n",
    "    tableArgGdpr = {\n",
    "        \"name\": tableName,\n",
    "        \"personal_data\": {\"hash\" : inputSchema}\n",
    "    }\n",
    "    return tableArgGdpr\n",
    "\n",
    "def genColumnSchema(tableDataframe : DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Generating the schema metadata for each column on a table\n",
    "    ...\n",
    "    Arguments\n",
    "    ----------\n",
    "    tableDataframe : DataFrame\n",
    "        a piece of the original dataframe that has the column \"table_name\" equal to a specific table name that enables us to see all the columns in that specific piece of dataframe\n",
    "    ...\n",
    "    Returns\n",
    "    ----------\n",
    "    Function returns a dictionary that contains metadata for each dataframe column (like: name, data_type, is_nullable, date_format etc)\n",
    "    \"\"\"\n",
    "    columnSchema = {\n",
    "        \"columns\": [],\n",
    "        \"hash\": []\n",
    "    }\n",
    "    for row in tableDataframe:\n",
    "        columnName = row.COLUMN_NAME.lower()\n",
    "        dataType = row.DATA_TYPE.lower()\n",
    "        if \"number\" in dataType:\n",
    "            dataType = dataType.replace(\"number\", \"decimal\")\n",
    "        elif \"varchar\" in dataType:\n",
    "            dataType = \"string\"\n",
    "        column_info = {\n",
    "            \"name\": columnName,\n",
    "            \"data_type\": dataType\n",
    "        }\n",
    "        if isNull(row.NULLABLE):\n",
    "            column_info[\"is_nullable\"] = True\n",
    "        if isDate(row.DATA_TYPE):\n",
    "            column_info[\"date_format\"] = \"yyyy-MM-dd HH:mm:ss\"\n",
    "        if isGdpr(row.GDPR_FLAG):\n",
    "            columnSchema[\"hash\"].append(columnName)\n",
    "        columnSchema[\"columns\"].append(column_info)\n",
    "    return columnSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Needed packages to run everything\"\"\"\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "\"\"\"All the functions to generate the schemas are called here and that data is saved in the variable \"schema\\\"\"\"\"\n",
    "\n",
    "def datasetTableInfo(dataFrame : DataFrame, metadataTitle : str, description : str = \"A metaschema generated from an example dataset.\") -> dict:\n",
    "    \"\"\"\n",
    "    All the pieces of the whole schema are generated here and are merged together\n",
    "    ...\n",
    "    Arguments\n",
    "    ----------\n",
    "    dataFrame : DataFrame\n",
    "        the dataframe that we are gonna generate all the schema for\n",
    "    metadataTitle : str\n",
    "        title of the schema file that we are gonna call it\n",
    "    description : str\n",
    "        description about the schema, if this argument is not sent the default value is used\n",
    "    ...\n",
    "    Returns\n",
    "    ----------\n",
    "    The whole generated schema about this dataframe\n",
    "    \"\"\"\n",
    "    sourceSystem = dataFrame.collect()[0].SOURCE_SYSTEM\n",
    "    completeSchema = {\n",
    "        \"metadata\": genPrimaryArgsMeta(metadataTitle, sourceSystem, description),\n",
    "        \"gdpr\": []\n",
    "    }\n",
    "    distinctTable_name = dataFrame.dropDuplicates([\"table_name\"]).collect()\n",
    "    for row in distinctTable_name:\n",
    "        tableName = row.table_name\n",
    "        singleTableDataframe = dataFrame.filter(dataFrame.table_name == tableName).collect()\n",
    "        sourceRef = createSourceSystemRef(row.FILE_NAME, row.DECIMAL_SEPARATOR, row.FILE_TYPE)\n",
    "        columnSchema = genColumnSchema(singleTableDataframe)\n",
    "        completeSchema[\"metadata\"][\"tables\"].append(genTableArgsMeta(tableName, sourceRef, columnSchema[\"columns\"]))\n",
    "        completeSchema[\"gdpr\"].append(genTableArgsGdpr(tableName, columnSchema[\"hash\"]))\n",
    "    return completeSchema\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\"\"\"Basic variables that define current stable version of schemas, \"SparkSession\" object, paths of where to read and save files and calling functions that generate and dump the schema to those files\"\"\"\n",
    "\n",
    "\"\"\"Gloabl varibles for major and minor stable metadata and gdpr versions\"\"\"\n",
    "majorMetadataVersion = 1\n",
    "minorMetadataVersion = 24\n",
    "metadata = generalMethods(majorMetadataVersion, minorMetadataVersion)\n",
    "\n",
    "majorGdprVersion = 1\n",
    "minorGdprVersion = 9\n",
    "stableGdprVersion = generalMethods(majorGdprVersion, minorGdprVersion)\n",
    "\n",
    "\"\"\"Object of SparkSession that is gonna be used for all the methods of pySpark\"\"\"\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "\"\"\"Global input metadata path\"\"\"\n",
    "inputDataFramePath = \"../Input CSV Metadata/20240101_metadata\"\n",
    "\n",
    "\"\"\"Global output metadata path\"\"\"\n",
    "outputSchemaPath = \"../Output Metadata/Schema Versions\"\n",
    "metadataFileName = \"Metadata\"\n",
    "gdprFileName = \"Gdpr\"\n",
    "\n",
    "\"\"\"Saving the dataframe into a variable so that we don't have to call the function readCSV() each time!\"\"\"\n",
    "dataFrame = spark.read.csv(f\"{inputDataFramePath}.csv\", header=True, sep=';').cache()\n",
    "\n",
    "\"\"\"Start the timer, not including the time it takes to read the csv\"\"\"\n",
    "beginTime = time.time()\n",
    "\n",
    "\"\"\"Generating the metadata from the dataset and saving it into a variable\"\"\"\n",
    "primaryArgSchema = datasetTableInfo(dataFrame, \"TestMetadata\")\n",
    "metadataSchema = primaryArgSchema[\"metadata\"]\n",
    "gdprSchema = primaryArgSchema[\"gdpr\"]\n",
    "\n",
    "\"\"\"Using generated metadata, output paths and the current latest stable version to dump the metadata into a .yaml schema \"\"\"\n",
    "metadata.writeMetadataFile(metadataSchema, outputSchemaPath, metadataFileName)\n",
    "metadata.writeMetadataFile(gdprSchema, outputSchemaPath, gdprFileName)\n",
    "\n",
    "\"\"\"Stop the spark session\"\"\"\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
